{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "GWgdNQfpYKRi"
      },
      "id": "GWgdNQfpYKRi"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from googletrans import Translator, LANGUAGES\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from deep_translator import GoogleTranslator\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "v-tghigpYN1H"
      },
      "id": "v-tghigpYN1H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "Bwna_DCnYZS7"
      },
      "id": "Bwna_DCnYZS7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712d8d95",
      "metadata": {
        "id": "712d8d95"
      },
      "outputs": [],
      "source": [
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U0001F300-\\U0001F5FF\"\n",
        "        u\"\\U0001F680-\\U0001F6FF\"\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove emojis\n",
        "    text = remove_emojis(text)\n",
        "\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('portuguese')).union(set(stopwords.words('english')))\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"Dataset.csv\")\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"Dataset.csv\")\n",
        "\n",
        "# Converting the 'comment' column to string\n",
        "df['comment'] = df['comment'].astype(str)\n",
        "\n",
        "# Applying the cleaning function\n",
        "df['comment'] = df['comment'].apply(remove_emojis)\n",
        "\n",
        "# Filtering comments with a maximum of 256 characters (after translation)\n",
        "df = df[df['comment'].str.len() <= 256]\n",
        "\n",
        "# Now your dataframe 'df' has clean comments, translated to English and with a maximum of 256 characters\n",
        "df = df[[\"id\", \"comment\", \"churn\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff7cfe2",
      "metadata": {
        "id": "8ff7cfe2"
      },
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "\n",
        "# Load Tokenizer and BERT Model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Dataset Class\n",
        "class ChurnDataset(Dataset):\n",
        "    def __init__(self, comments, labels):\n",
        "        self.comments = comments\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.comments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        comment = self.comments[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            comment,\n",
        "            add_special_tokens=True,\n",
        "            max_length=256,\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'comment_text': comment,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# DataLoaders\n",
        "train_dataset = ChurnDataset(train_df['comment'].to_numpy(), train_df['churn'].to_numpy())\n",
        "test_dataset = ChurnDataset(test_df['comment'].to_numpy(), test_df['churn'].to_numpy())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# ChurnClassifier Class\n",
        "class ChurnClassifier(nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super(ChurnClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_output = self.sigmoid(linear_output)\n",
        "        return final_output\n",
        "\n",
        "# Initialize the Model\n",
        "model = ChurnClassifier(model_bert)\n",
        "\n",
        "# Training\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Function to extract embeddings from BERT\n",
        "def extract_embeddings(dataloader, model):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            label = batch['labels']\n",
        "\n",
        "            _, pooled_output = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "            embeddings.extend(pooled_output.detach().cpu().numpy())\n",
        "            labels.extend(label.detach().cpu().numpy())\n",
        "\n",
        "    return np.array(embeddings), np.array(labels)\n",
        "\n",
        "# Extract embeddings for the training and test sets\n",
        "train_embeddings, train_labels = extract_embeddings(train_loader, model_bert)\n",
        "test_embeddings, test_labels = extract_embeddings(test_loader, model_bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "MNqV1o8dfzJh"
      },
      "id": "MNqV1o8dfzJh"
    },
    {
      "cell_type": "markdown",
      "id": "615a8c5e",
      "metadata": {
        "id": "615a8c5e"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6d8d48e",
      "metadata": {
        "id": "d6d8d48e"
      },
      "outputs": [],
      "source": [
        "\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Hyperparameter space for the search\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Define the scorer\n",
        "scorer = make_scorer(f1_score, average='binary')  # or use scorer=make_scorer(accuracy_score) for accuracy\n",
        "\n",
        "# Instantiate the GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, scoring=scorer, cv=5, verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit the model with the dataset\n",
        "grid_search.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Best score\n",
        "print(\"Best score:\", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1e56f6b",
      "metadata": {
        "id": "f1e56f6b"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b07b8bb",
      "metadata": {
        "id": "5b07b8bb"
      },
      "outputs": [],
      "source": [
        "svm = SVC()\n",
        "\n",
        "# Hyperparameter space for SVM search\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],  # Regularization\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'degree': [2, 3, 4]\n",
        "}\n",
        "\n",
        "# GridSearchCV for SVM\n",
        "grid_search_svm = GridSearchCV(estimator=svm, param_grid=param_grid_svm, scoring=scorer, cv=5, verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit the SVM model with the dataset\n",
        "grid_search_svm.fit(train_embeddings, train_labels)\n",
        "\n",
        "# Best parameters for SVM\n",
        "print(\"Best hyperparameters for SVM:\", grid_search_svm.best_params_)\n",
        "\n",
        "# Best score for SVM\n",
        "print(\"Best score for SVM:\", grid_search_svm.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de7e4778",
      "metadata": {
        "id": "de7e4778"
      },
      "source": [
        "### MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7fb3665",
      "metadata": {
        "id": "b7fb3665"
      },
      "outputs": [],
      "source": [
        "mlp = MLPClassifier()\n",
        "\n",
        "# Hyperparameter space for MLP search\n",
        "param_grid_mlp = {\n",
        "    'hidden_layer_sizes': [(50, 50), (100, 100), (50, 100, 50)],  # Varied sizes for hidden layers\n",
        "    'activation': ['tanh', 'relu', 'logistic'],  # Different activation functions\n",
        "    'solver': ['sgd', 'adam', 'lbfgs'],  # Different algorithms for weight optimization\n",
        "    'alpha': [0.0001, 0.001, 0.05],  # Varied values for L2 regularization term\n",
        "    'learning_rate': ['constant', 'adaptive', 'invscaling']  # Different learning rate strategies\n",
        "}\n",
        "\n",
        "# Define the scorers\n",
        "scoring = {\n",
        "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
        "    'accuracy': 'accuracy',\n",
        "}\n",
        "\n",
        "#\n",
        "grid_search_mlp = GridSearchCV(estimator=mlp, param_grid=param_grid_mlp, scoring=scoring, refit='f1_macro', cv=5, verbose=2, n_jobs=-1)\n",
        "\n",
        "#\n",
        "grid_search_mlp.fit(train_embeddings, train_labels)\n",
        "\n",
        "#Show the results\n",
        "print(\"Best hyperparameters for MLP:\", grid_search_mlp.best_params_)\n",
        "print(\"Best macro f1-score for MLP:\", grid_search_mlp.best_score_)\n",
        "\n",
        "print(\"Best accuracy for MLP:\", grid_search_mlp.cv_results_['mean_test_accuracy'][grid_search_mlp.best_index_])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f93b06ca",
      "metadata": {
        "id": "f93b06ca"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GWgdNQfpYKRi",
        "Bwna_DCnYZS7",
        "615a8c5e",
        "f1e56f6b",
        "de7e4778"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}