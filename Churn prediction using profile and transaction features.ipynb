{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg-8YIBW9_q9"
      },
      "source": [
        "# Churn prediction using profile and transaction features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rc9OvDiCNq2"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vccfStyWCO8l"
      },
      "outputs": [],
      "source": [
        "# Data Manipulation and Analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# File Operations\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "# Data Preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "\n",
        "# Machine Learning Models\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Dataset\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Model Evaluation and Selection\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
        "\n",
        "# Other\n",
        "import datetime as dt\n",
        "import re\n",
        "from sklearn.model_selection import ParameterGrid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46guJhNz-drL"
      },
      "source": [
        "## Data Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XztPllkz_ctQ"
      },
      "source": [
        "### Import data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLQ51xzP_g1x"
      },
      "outputs": [],
      "source": [
        "# Uploading files from the local system to the Colab notebook\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EML6WVdB_o62"
      },
      "outputs": [],
      "source": [
        "## ↓ Import Food Delivery Dataset ↓\n",
        "df = pd.read_csv('fooddelivery.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMFpQ8-hA0xf"
      },
      "source": [
        "### Exploratory data analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou-EWv7dBJKr"
      },
      "outputs": [],
      "source": [
        "# view information about the dataset\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM-ELAKUA1ZK"
      },
      "outputs": [],
      "source": [
        "# view dimensions of dataset\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrYPDI81BBog"
      },
      "outputs": [],
      "source": [
        "# let's preview the dataset\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnDv4jqGA6qC"
      },
      "outputs": [],
      "source": [
        "# view the column names of the dataframe\n",
        "\n",
        "col_names = df.columns\n",
        "\n",
        "col_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTiI96nDBGZ2"
      },
      "outputs": [],
      "source": [
        "# remove leading spaces from column names\n",
        "\n",
        "df.columns = df.columns.str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ_wAFBRBIQt"
      },
      "outputs": [],
      "source": [
        "# view column names again\n",
        "\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYcilDZoBwLc"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlwJ6AU3B5rn"
      },
      "outputs": [],
      "source": [
        "# check for missing values in variables\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7oSn_WlCgeC"
      },
      "outputs": [],
      "source": [
        "# Replacing null or empty values with 0 in all columns\n",
        "df.fillna(0.0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HRuCt1LCkkS"
      },
      "outputs": [],
      "source": [
        "# iterate over the columns of the DataFrame\n",
        "for col in df.columns:\n",
        "    # check if the column is of type \"int\"\n",
        "    if df[col].dtype == int:\n",
        "        # convert the column to \"float\"\n",
        "        df[col] = df[col].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgshceENCnwM"
      },
      "outputs": [],
      "source": [
        "# view summary statistics in numerical variables\n",
        "round(df.describe(),2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgDVYW-8DegN"
      },
      "source": [
        "### RFM Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6feyz4XBDRpc"
      },
      "outputs": [],
      "source": [
        "# Food Delivery-----------------------------------------------------\n",
        "df_RFM = df[['customerId','DaysLastOrder', 'orderQuantity', 'totalValue']].copy()\n",
        "\n",
        "df_RFM.rename(columns={\n",
        "    'DaysLastOrder': 'Recency',\n",
        "    'orderQuantity': 'Frequency',\n",
        "    'totalValue': 'MonetaryValue'\n",
        "}, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Calculate quartiles for each RFM metric\n",
        "quartiles = df_RFM.quantile(q=[0.20, 0.40, 0.60, 0.80])\n",
        "\n",
        "# Define function to assign quartile scores\n",
        "def assign_quartile_score(value, quartile):\n",
        "    if value <= quartile[0.20]:\n",
        "        return 1\n",
        "    elif value <= quartile[0.40]:\n",
        "        return 2\n",
        "    elif value <= quartile[0.60]:\n",
        "        return 3\n",
        "    elif value <= quartile[0.80]:\n",
        "        return 4\n",
        "    else:\n",
        "        return 5\n",
        "\n",
        "# Assign quartile scores to each RFM metric\n",
        "df_RFM['R'] = df_RFM['Recency'].apply(assign_quartile_score, args=(quartiles['Recency'],))\n",
        "df_RFM['F'] = df_RFM['Frequency'].apply(assign_quartile_score, args=(quartiles['Frequency'],))\n",
        "df_RFM['M'] = df_RFM['MonetaryValue'].apply(assign_quartile_score, args=(quartiles['MonetaryValue'],))\n",
        "\n",
        "# Concatenate RFM scores to create a combined RFM score\n",
        "df_RFM['RFM_score'] = df_RFM['R'].map(str) + df_RFM['F'].map(str) + df_RFM['M'].map(str)\n",
        "\n",
        "# Define dictionary to map RFM scores to segments using regular expressions\n",
        "rfm_segment_dict = {\n",
        "    'Not_Fan': [r'[1][1-5]', r'[2][1-2]'],\n",
        "    'Switchers': [r'[2][3-5]', r'[3][1-2]', r'[4-5][1-2]'],\n",
        "    'Loyal': [r'[3][3-5]', r'[4-5][3]'],\n",
        "    'Champions': [r'[4-5][4-5]']\n",
        "}\n",
        "\n",
        "# Function to get segment for a given RFM score\n",
        "def get_segment(rfm_score):\n",
        "    segment = None\n",
        "    for seg, patterns in rfm_segment_dict.items():\n",
        "        for pattern in patterns:\n",
        "            if re.match(pattern, rfm_score):\n",
        "                segment = seg\n",
        "                break\n",
        "        if segment is not None:\n",
        "            break\n",
        "    return segment\n",
        "\n",
        "# Add RFM segment column to the dataframe\n",
        "df_RFM['RFM_segment'] = df_RFM['RFM_score'].apply(get_segment)\n",
        "df_RFM = df_RFM[['customerId','RFM_segment']]\n",
        "\n",
        "df = pd.merge(df, df_RFM, how = 'left', on = 'customerId')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsu4ddHuDq9V"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPBT6nkNyxRv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Suppose df is your DataFrame containing the data\n",
        "\n",
        "# Delete invalid columns\n",
        "df = df.drop('customerId', axis=1)\n",
        "df = df.drop('DaysLastOrder', axis=1)\n",
        "\n",
        "# Identify data types\n",
        "data_types = df.dtypes\n",
        "print(\"\\nData Types:\")\n",
        "print(data_types)\n",
        "\n",
        "# Perform data cleaning and preprocessing\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':  # Handle categorical columns\n",
        "        # Convert string columns to lowercase\n",
        "        df[column] = df[column].str.lower()\n",
        "\n",
        "        # Replace missing values with the most frequent value\n",
        "        mode = df[column].mode().iloc[0]\n",
        "        df[column].fillna(mode, inplace=True)\n",
        "\n",
        "# Use get_dummies to perform one-hot encoding for all categorical columns\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "for column in df.columns:\n",
        "    if df[column].dtype in ['float64', 'float32', 'int64', 'int32']:  # Handle numerical columns\n",
        "        # Replace missing values with the mean\n",
        "        mean = df[column].mean()\n",
        "        df[column].fillna(mean, inplace=True)\n",
        "\n",
        "        # Normalize numerical columns using Min-Max scaling\n",
        "        scaler = MinMaxScaler()\n",
        "        df[column] = scaler.fit_transform(df[column].values.reshape(-1, 1))\n",
        "    else:\n",
        "        # Handle other data types as needed\n",
        "        pass\n",
        "\n",
        "# Set the target variable as 'Churn'\n",
        "y = df['churn']\n",
        "X = df.drop('churn', axis=1)\n",
        "\n",
        "# Display the cleaned and preprocessed dataset\n",
        "print(\"\\nCleaned and Preprocessed Dataset:\")\n",
        "print(X.head())\n",
        "print(\"\\nTarget variable:\")\n",
        "print(y.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbSbFVL51THO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Display the loaded dataset\n",
        "print(\"Loaded Dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Identify data types\n",
        "data_types = df.dtypes\n",
        "print(\"\\nData Types:\")\n",
        "print(data_types)\n",
        "\n",
        "# Perform data cleaning and preprocessing\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':  # Handle categorical columns\n",
        "        # Convert string columns to lowercase\n",
        "        df[column] = df[column].str.lower()\n",
        "\n",
        "        # Replace missing values with the most frequent value\n",
        "        mode = df[column].mode().iloc[0]\n",
        "        df[column].fillna(mode, inplace=True)\n",
        "\n",
        "        # Check if the column is still a categorical variable\n",
        "        if df[column].dtype == 'object':\n",
        "            # One-hot encode categorical columns using OneHotEncoder\n",
        "            encoder = OneHotEncoder()\n",
        "            encoded_columns = pd.DataFrame(encoder.fit_transform(df[[column]]).toarray(),\n",
        "                                           columns=[f\"{column}_{val}\" for val in encoder.categories_[0]])\n",
        "            df = pd.concat([df, encoded_columns], axis=1)\n",
        "            df.drop(column, axis=1, inplace=True)\n",
        "\n",
        "    elif df[column].dtype in ['float64', 'float32', 'int64', 'int32']:  # Handle numerical columns\n",
        "        # Replace missing values with the mean\n",
        "        mean = df[column].mean()\n",
        "        df[column].fillna(mean, inplace=True)\n",
        "\n",
        "        # Normalize numerical columns using Min-Max scaling\n",
        "        scaler = MinMaxScaler()\n",
        "        df[column] = scaler.fit_transform(df[column].values.reshape(-1, 1))\n",
        "    else:\n",
        "        # Handle other data types as needed\n",
        "        pass\n",
        "\n",
        "# Set the target variable as 'Churn'\n",
        "y = df['churn']\n",
        "X = df.drop('churn', axis=1)\n",
        "\n",
        "# Display the cleaned and preprocessed dataset\n",
        "print(\"\\nCleaned and Preprocessed Dataset:\")\n",
        "print(X.head())\n",
        "print(\"\\nTarget variable:\")\n",
        "print(y.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQUwWaELECre"
      },
      "source": [
        "## Quick Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dibxvyzRENkt"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    'MLP': MLPClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'Decision Trees': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier()\n",
        "}\n",
        "\n",
        "# Function to train and evaluate a model\n",
        "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "test_size = 0.2  # set the test size\n",
        "random_state = 42  # set the random state\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "# Loop over all models and evaluate their performance\n",
        "accuracies = []\n",
        "for model_name, model in models.items():\n",
        "    accuracy, precision, recall, f1 = train_and_evaluate(model, X_train, X_test, y_train, y_test)\n",
        "    accuracies.append(accuracy)\n",
        "    print(\"--------------------\")\n",
        "    print(\"Model: \", model_name)\n",
        "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "    print(\"Precision: {:.2f}%\".format(precision * 100))\n",
        "    print(\"Recall: {:.2f}%\".format(recall * 100))\n",
        "    print(\"F1 Score: {:.2f}%\".format(f1 * 100))\n",
        "    print(\"--------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1Ou621NElGY"
      },
      "source": [
        "## Filter: highest accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbiXcSq86cM7"
      },
      "outputs": [],
      "source": [
        "# List to store the selected models\n",
        "selected_models = []\n",
        "\n",
        "# Calculate the mean accuracy\n",
        "mean_accuracy = np.mean(accuracies)\n",
        "print(\"Mean Accuracy: {:.2f}%\".format(mean_accuracy * 100))\n",
        "\n",
        "# Iterate over the model names and their corresponding accuracies\n",
        "for model_name, accuracy in zip(models.keys(), accuracies):\n",
        "    MAD = mean_accuracy - accuracy  # Calculate the difference from the mean accuracy\n",
        "    print(\"--------------------\")\n",
        "    print(\"Model: \", model_name)\n",
        "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "    print(\"Difference from Mean Accuracy: {:.2f}%\".format(MAD * 100))\n",
        "\n",
        "    # Check if the difference from the mean accuracy exceeds the threshold of 0.025\n",
        "    if MAD > 0.025:\n",
        "        print(\"--------------------\")\n",
        "        print(\"Tag: Does not enter the next phase\")\n",
        "    else:\n",
        "        selected_models.append(model_name)  # Add the model to the selected models list\n",
        "    print(\"--------------------\")\n",
        "\n",
        "# Print the list of selected models\n",
        "print(\"Selected Models:\", selected_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkN82iMrEq2L"
      },
      "source": [
        "## Hyperparameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5obqrD4eRpl"
      },
      "outputs": [],
      "source": [
        "# Define the machine learning models\n",
        "research_models = {'MLP': MLPClassifier(),\n",
        "          'SVM': SVC(),\n",
        "          'Decision Trees': DecisionTreeClassifier(),\n",
        "          'Random Forest': RandomForestClassifier()}\n",
        "\n",
        "models = {chave: valor for chave, valor in research_models.items() if chave in selected_models}\n",
        "\n",
        "# Define hyperparameters for each model\n",
        "model_params = {\n",
        "    'MLP': {\n",
        "        'hidden_layer_sizes': [(3,), (5,), (10,), (64,)],\n",
        "        'max_iter': [2000],\n",
        "        'alpha': [0.001, 0.01],\n",
        "        'solver': ['sgd', 'adam'],\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'verbose': [False]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'kernel': ['linear', 'rbf', 'poly'],\n",
        "        'C': [0.1, 1, 10],\n",
        "        'gamma': ['auto', 'scale']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [50],\n",
        "        'criterion': ['entropy'],\n",
        "        'max_depth': [10],\n",
        "        'random_state': [None]\n",
        "    },\n",
        "    'Decision Trees': {\n",
        "        'criterion': ['entropy'],\n",
        "        'max_depth': [3],\n",
        "        'min_samples_split': [10],\n",
        "        'min_samples_leaf': [2]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate selected models with cross-validation\n",
        "best_params = {}\n",
        "\n",
        "# Train and evaluate selected models with cross-validation\n",
        "for model_name in models:\n",
        "    model = models[model_name]\n",
        "    params = model_params[model_name]\n",
        "    best_accuracy = 0\n",
        "    best_params[model_name] = None\n",
        "    for param_set in ParameterGrid(params):\n",
        "        clf = model.set_params(**param_set)\n",
        "        scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
        "        accuracy = scores.mean()\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_params[model_name] = param_set\n",
        "    clf = model.set_params(**best_params[model_name])\n",
        "    clf.fit(X_train, y_train)\n",
        "    acc, prec, rec, f1 = train_and_evaluate(clf, X_train, X_test, y_train, y_test)\n",
        "    print(\"--------------------\")\n",
        "    print(f\"Best hyperparameters for {model_name}: {best_params[model_name]}\")\n",
        "    print(f\"Acurácia para {model_name}: {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNKQUwpJFopX"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKz1PncbFqpF"
      },
      "outputs": [],
      "source": [
        "best_model = None\n",
        "best_score = 0\n",
        "\n",
        "# Instantiation and training of the models that are in the 'best_params' dictionary\n",
        "for model, params in best_params.items():\n",
        "    if model == 'Decision Trees':\n",
        "        clf = DecisionTreeClassifier(**params)\n",
        "    elif model == 'Random Forest':\n",
        "        clf = RandomForestClassifier(**params)\n",
        "    elif model == 'SVM':\n",
        "        clf = SVC(**params)\n",
        "    elif model == 'MLP':\n",
        "        clf = MLPClassifier(**params)\n",
        "    else:\n",
        "        raise ValueError(f\"Unrecognized model: '{model}'.\")\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    score = clf.score(X_test, y_test)\n",
        "    print(f\"Accuracy of the {model} model: {score:.4f}\")\n",
        "\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_model = clf\n",
        "\n",
        "# import joblib\n",
        "# ↓ If you want to save the best model trained uncomment ↓\n",
        "#joblib.dump(best_model, 'best_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbQ_gpnbFrX2"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzPbXfZLky8S"
      },
      "outputs": [],
      "source": [
        "# Creation of a dictionary with the best parameters of each model\n",
        "selected_models_evaluation = best_params\n",
        "\n",
        "# Select the model for evaluation\n",
        "print(\"Select the model for evaluation:\")\n",
        "for model in selected_models_evaluation.keys():\n",
        "    print(f\"- {model}\")\n",
        "\n",
        "# ask the user to choose a template\n",
        "chosen_model = input(\"Model: \")\n",
        "\n",
        "# Verification if the chosen model is in the dictionary of best parameters\n",
        "if chosen_model in best_params:\n",
        "    # Instantiation and training of the chosen model\n",
        "    params = best_params[chosen_model]\n",
        "    if chosen_model == 'Decision Trees':\n",
        "        clf = DecisionTreeClassifier(**params)\n",
        "    elif chosen_model == 'Random Forest':\n",
        "        clf = RandomForestClassifier(**params)\n",
        "    elif chosen_model == 'SVM':\n",
        "        clf = SVC(**params)\n",
        "    elif chosen_model == 'MLP':\n",
        "        clf = MLPClassifier(**params)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluation of the chosen model and printing of the accuracy\n",
        "    score = clf.score(X_test, y_test)\n",
        "\n",
        "    # Verification if the chosen model is the best so far\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_model = clf\n",
        "else:\n",
        "    raise ValueError(f\"Unrecognized model: '{chosen_model}'.\")\n",
        "\n",
        "y_pred = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56tRZ9ViMDYg"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)  # Compute the confusion matrix using predicted and true labels\n",
        "\n",
        "print('Confusion matrix:\\n\\n', cm)  # Print the confusion matrix\n",
        "\n",
        "tn, fp, fn, tp = cm.ravel()  # Unpack the elements of the confusion matrix into separate variables\n",
        "\n",
        "print('\\nTrue Positives(TP) =', tp)  # Print the number of true positives\n",
        "\n",
        "print('True Negatives(TN) =', tn)  # Print the number of true negatives\n",
        "\n",
        "print('False Positives(FP) =', fp)  # Print the number of false positives\n",
        "\n",
        "print('False Negatives(FN) =', fn)  # Print the number of false negatives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAqgSjydFuKe"
      },
      "outputs": [],
      "source": [
        "# visualize confusion matrix with seaborn heatmap\n",
        "\n",
        "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'],\n",
        "                                 index=['Predict Positive:1', 'Predict Negative:0'])\n",
        "\n",
        "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijf6XYIphEUG"
      },
      "outputs": [],
      "source": [
        "# Generate a classification report by comparing the true labels (y_test) with the predicted labels (y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(report)  # Print the classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuBKUTu7hJlV"
      },
      "outputs": [],
      "source": [
        "TP = cm[0, 0]  # Assign the value at the top-left position of the confusion matrix to TP\n",
        "TN = cm[1, 1]  # Assign the value at the bottom-right position of the confusion matrix to TN\n",
        "FP = cm[0, 1]  # Assign the value in the first row and second column of the confusion matrix to FP\n",
        "FN = cm[1, 0]  # Assign the value in the second row and first column of the confusion matrix to FN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LC6zS10hJIJ"
      },
      "outputs": [],
      "source": [
        "# print classification accuracy\n",
        "\n",
        "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
        "\n",
        "print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwKXJnBbhIii"
      },
      "outputs": [],
      "source": [
        "# print classification error\n",
        "\n",
        "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
        "\n",
        "print('Classification error : {0:0.4f}'.format(classification_error))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrLsFwGChPhQ"
      },
      "outputs": [],
      "source": [
        "# print precision score\n",
        "\n",
        "precision = TP / float(TP + FP)\n",
        "\n",
        "\n",
        "print('Precision : {0:0.4f}'.format(precision))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k1Ja2DrhRyS"
      },
      "outputs": [],
      "source": [
        "# print recall score\n",
        "\n",
        "recall = TP / float(TP + FN)\n",
        "\n",
        "print('Recall or Sensitivity : {0:0.4f}'.format(recall))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdeVztYUhUT4"
      },
      "outputs": [],
      "source": [
        "# print true_positive_rate score\n",
        "\n",
        "true_positive_rate = TP / float(TP + FN)\n",
        "\n",
        "\n",
        "print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY4HJyAWhXkQ"
      },
      "outputs": [],
      "source": [
        "# print false_positive_rate score\n",
        "\n",
        "false_positive_rate = FP / float(FP + TN)\n",
        "\n",
        "\n",
        "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tSHDRbbhYTm"
      },
      "outputs": [],
      "source": [
        "# print specificity score\n",
        "\n",
        "specificity = TN / (TN + FP)\n",
        "\n",
        "print('Specificity : {0:0.4f}'.format(specificity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI6LcanHhcC6"
      },
      "outputs": [],
      "source": [
        "# plot ROC Curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "\n",
        "plt.plot(fpr, tpr, linewidth=2)\n",
        "\n",
        "plt.plot([0,1], [0,1], 'k--' )\n",
        "\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "plt.title('ROC curve for Predicting a Churn')\n",
        "\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZUBv_aehe8e"
      },
      "outputs": [],
      "source": [
        "# compute ROC AUC\n",
        "\n",
        "ROC_AUC = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
